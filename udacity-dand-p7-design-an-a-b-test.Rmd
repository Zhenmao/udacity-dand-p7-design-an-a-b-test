---
title: "Design an A/B Test"
output: 
  html_document: 
    number_sections: yes
    self_contained: no
    toc: yes
---

```{r, message=FALSE, packages}
```

# Introduction

At the time of this experiment, Udacity courses currently have two options on the course overview page: "start free trial", and "access course materials". If the student clicks "start free trial", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks "access course materials", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.

In the experiment, Udacity tested a change where if the student clicked "start free trial", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. [This screenshot](https://www.google.com/url?q=https://drive.google.com/a/knowlabs.com/file/d/0ByAfiG8HpNUMakVrS0s4cGN2TjQ/view?usp%3Dsharing&sa=D&ust=1505462930089000&usg=AFQjCNHbTwpTCPD-bpNwBgyNz8Exgzif1A) shows what the experiment looks like.

The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough time—without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.

The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.

# Experiment Design

## Metric Choice

**Invariant Metrics**:

* **Number of cookies**: The number of unique cookies to view the course overview page (# cookies) should be an invariant metric. This step happens before the free trial screener step. Therefore, the number of cookies should not change across the experiment group and the control group.
* **Number of clicks**: The number of unique cookies to click the "Start free trial” button (# clicks) should be an invariant metric. This step happens before the free trial screener step. Therefore, the number of clicks should not change across the experiment group and the control group.
* **Click-through-probability**: The number of unique cookies to click the "Start free trial" button (# clicks) divided by number of unique cookies to view the course overview page (# cookies) should be an invariant metric. Because both the numerator (# clicks) and the denominator (# cookies) do not change, the ratio (click-through-probability) should also not change across the experiment group and the control group.

**Evaluation Metrics**:

* **Gross conversion**: The number of user-ids to complete checkout and enroll in the free trial (# user-ids) divided by number of unique cookies to click the "Start free trial” button (# clicks) should be an evaluation metric. The added time survey step should steer away users who do not have enough time for studying, and decrease the # trial user-ids. Because the numerator (# user-ids) should decrease and the denominator (# clicks) should be invariant, the ratio (gross conversion) should decrease for the experiment group.
* **Net conversion**: The number of user-ids to remain enrolled past the 14-day boundary (# remained user-ids) divided by the number of unique cookies to click the "Start free trial” button (# clicks) should be an evaluation metric. The numerator (# remained user-ids) is expected to be higher in experiment group as a result of increased overall experience, and the denominator (# clicks) should be invariant, therefore the ratio (net conversion) should be higher in the experiment group.

**Metrics Not Chosen**:

* **Number of user-ids**: The number of users who enroll in the free trial (# user-ids) is not chosen as included metrics. Although # trial user-ids is expected to be reduced in the experiment group according to our hypothesis, we cannot guarantee that equal number of cookies are assigned into experiment and control group. This limitation makes the # trial user-ids less suitable to be used as an evaluation metric compared to a normalized metric such as gross conversion.
* **Retention**: The number of user-ids to remain enrolled past the 14-day boundary (# remained user-ids) divided by number of user-ids to complete checkout (# user-ids) was initially chosen as evaluation metrics. The retention is expected to be higher in the experiment group as a result of increased overall experience. However, after calculating the duration that needed for testing this metric, the duration is too long to be practical running this experiment. Therefore, retention is not chosen for the final experiment design.

In order to launch the experiment, the experiment results need to align with the two goals:

1. Reduce the number of frustrated students who left the free trial because they didn't have enough time.
2. Do not significantly reduce the number of students to continue past the free trial and eventually complete the course.

These two goals correspond to the following two chosen evaluation metrics changes:

1. Decreased gross conversion.
2. Increase or non-changed net conversion.

To launch the experiment, one should require to see **BOTH** decreased gross conversion **AND**
increased or non-changed net conversion.

## Measuring Standard Deviation

Given a sample size of 5000 cookies visiting the course overview page

Rough estimates of the baseline values for these metrics:

| Metric Name                                          | Value     |
|------------------------------------------------------|-----------|
| Unique cookies to view course overview page per day  | 40000     |
| Unique cookies to click "Start free trial" per day   | 3200      |
| Enrollments per day                                  | 660       |
| Click-through-probability on "Start free trial"      | 0.08      |
| Probability of enrolling, given click                | 0.20625   |
| Probability of payment, given enroll                 | 0.53      |
| Probability of payment, given click                  | 0.1093125 |

```{r}
binomSD <- function(p, n) {
  return(sqrt(p * (1-p) / n))
}
```

```{r}
# Number of clicks for the given sample
n_clicks <- 5000 * (3200 / 40000)
# Gross conversion
p_gross_conversion <- 0.20625
binomSD(p_gross_conversion, n_clicks)
```
Gross conversion standard deviation: 0.0202. The analytic estimate would be comparable to the empirical variability, because the unit of diversion and the unit of analysis are the same: a cookie.

```{r}
# Net conversion
p_net_conversion <- 0.1093125
binomSD(p_net_conversion, n_clicks)
```
Net conversion standard deviation: 0.0156. The analytic estimate would be comparable to the empirical variability, because the unit of diversion and the unit of analysis are the same: a cookie.

## Sizing

### Number of Samples vs. Power
```{r, required_sample_size}
## Strategy: For a bunch of Ns, compute the z_star by achieving desired alpha, then
## compute what beta would be for that N using the acquired z_star. 
## Pick the smallest N at which beta crosses the desired value

# Inputs:
#   The desired alpha for a two-tailed test
# Returns: The z-critical value
get_z_star <- function(alpha) {
    return(-qnorm(alpha / 2))
}

# Inputs:
#   z-star: The z-critical value
#   s: The standard error of the metric at N=1
#   d_min: The practical significance level
#   N: The sample size of each group of the experiment
# Returns: The beta value of the two-tailed test
get_beta <- function(z_star, s, d_min, N) {
    SE <- s /  sqrt(N)
    return(pnorm(z_star * SE, mean=d_min, sd=SE))
}

# Inputs:
#   s: The standard error of the metric with N=1 in each group
#   d_min: The practical significance level
#   Ns: The sample sizes to try
#   alpha: The desired alpha level of the test
#   beta: The desired beta level of the test
# Returns: The smallest N out of the given Ns that will achieve the desired
#          beta. There should be at least N samples in each group of the experiment.
#          If none of the given Ns will work, returns -1. N is the number of
#          samples in each group.

required_size <- function(s, d_min, Ns=1:20000, alpha=0.05, beta=0.2) {
    for (N in Ns) {
        if (get_beta(get_z_star(alpha), s, d_min, N) <= beta) {
            return(N)
        }
    }
    
    return(-1)
}
```

I will not use the Bonferroni correction during my analysis. Use an alpha of 0.05 and a beta of 0.2.

For dectecing gross conversion difference of the practical significance boundary $d_\text{min}=0.01$, the required number of pageviews is 642475:
```{r}
# Required sample size for gross conversion
pageviews_gross_conversion <- required_size(s=sqrt(0.20625*(1-0.20625)*2), d_min=0.01, alpha=0.05, Ns=1:1000000)*(40000/3200)*2
pageviews_gross_conversion
```

For detecting net conversion difference reaches the practical significance boundary $d_\text{min}=0.0075$, the required number of pageviews is 679300:
```{r}
# Required smaple size for net conversion
pageviews_net_conversion <- required_size(s=sqrt(0.1093125*(1-0.1093125)*2), d_min=0.0075,  alpha=0.05, Ns=1:1000000)*(40000/3200)*2
pageviews_net_conversion
```

Because the experiment needs to be powerful enough to detect both metrics reaching practical significance boundaries, I need **679300** pageviews in total to collect to adequately power the experiment.
```{r}
pageviews_required <- max(pageviews_gross_conversion, pageviews_net_conversion)
pageviews_required
```


### Choosing Duration vs. Exposure

I think this experiment is of low risk and the number of page views needed is fairly large, therefore diverting all traffic to the experiment can help finish the experiment in a timely fashion in **17** days.
```{r}
# Number of days needed if diverting all traffic to the experiment
duration <- ceiling(pageviews_required / 40000)
duration
```

The risk of the experiment is low, because:

1. The free trial screener is only one added small step that does not affect other existing pages. Even if the experiment fails and the free trial screener will not roll out, it will not affect the users in the experiment group in the future.
2. The additional data collected--how much time they have available to devote to the course--is not sensitive. And no any other sensitive information is collected.
3. The experiment will not harm the participants in any way.

# Experiment Analysis

## Sanity Checks
```{r}
# Import csv data
data_control <- read.csv('data_control.csv', row.names=1)
data_experiment <- read.csv('data_experiment.csv', row.names=1)
```
The meaning of each column is:

* Pageviews: Number of unique cookies to view the course overview page that day.
* Clicks: Number of unique cookies to click the course overview page that day.
* Enrollments: Number of user-ids to enroll in the free trial that day.
* Payments: Number of user-ids who who enrolled on that day to remain enrolled for 14 days and thus make a payment.

```{r}
# Inputs:
#   var_control: A dataframe column containing the control group data
#   var_experiment: A dataframe column containing the experiment group data
# Returns: The observed fraction assgined to the control group 
observed_counts_fraction <- function(var_control, var_experiment) {
  return(sum(var_control) / (sum(var_control) + sum(var_experiment)))
}

# Inputs:
#   var_control_numerator: A dataframe column containing the control group data numerator
#   var_control_denominator: A dataframe column containing the control group data denominator
#   var_experiment_numerator: A dataframe column containing the experiment group data numerator
#   var_experiment_demoninator: A dataframe column containing the experiment group data denominator
# Returns: The observed proportion difference between groups
observed_proportion_difference <- function(var_control_numerator, var_control_denominator, var_experiment_numerator, var_experiment_demoninator) {
  return(sum(var_experiment_numerator)/sum(var_experiment_demoninator) - sum(var_control_numerator)/sum(var_control_denominator))
}

# Inputs:
#   var_control: A dataframe column containing the control group data
#   var_experiment: A dataframe column containing the experiment group data
#   alpha: The desired alpha level
# Returns: The 95% confidence interval for the expected fraction assigned to the control group 
ci_counts_fraction <- function(var_control, var_experiment, alpha=0.05) {
  p <- 0.5
  z_star <- get_z_star(alpha)
  SE <- sqrt(p*(1-p) / (sum(var_control) + sum(var_experiment)))
  return(c(p - z_star * SE, p + z_star * SE))
}

# Inputs:
#   var_control_numerator: A dataframe column containing the control group data numerator
#   var_control_denominator: A dataframe column containing the control group data denominator
#   var_experiment_numerator: A dataframe column containing the experiment group data numerator
#   var_experiment_demoninator: A dataframe column containing the experiment group data denominator
#   alpha: The desired alpha level
# Returns: The 95% confidence interval for the expected proportion difference between groups
ci_proportion_difference <- function(var_control_numerator, var_control_denominator, var_experiment_numerator, var_experiment_demoninator, alpha=0.05) {
  p_pool <- (sum(var_control_numerator) + sum(var_experiment_numerator)) / (sum(var_control_denominator) + sum(var_experiment_demoninator))
  z_star <- get_z_star(alpha)
  SE <- sqrt(p_pool*(1-p_pool)*(1/sum(var_control_denominator) + 1/sum(var_experiment_demoninator)))
  return(c(-z_star * SE, z_star * SE))
}

# Input:
#   observed: The observed value
#   ci: The 95% confidence interval
# Returns: A bool value indicating whether passing sanity check
pass_sanity_check <- function(observed, ci) {
  return(observed>ci[1] & observed<ci[2])
}
```

**Number of cookies**

```{r}
observed_n_cookies <- observed_counts_fraction(data_control$Pageviews, data_experiment$Pageviews)
observed_n_cookies
```
```{r}
ci_n_cookies <- ci_counts_fraction(data_control$Pageviews, data_experiment$Pageviews)
ci_n_cookies
```
```{r}
pass_sanity_check(observed_n_cookies, ci_n_cookies)
```

**Number of clicks**
```{r}
observed_n_clicks <- observed_counts_fraction(data_control$Clicks, data_experiment$Clicks)
observed_n_clicks
```
```{r}
ci_n_clicks <- ci_counts_fraction(data_control$Clicks, data_experiment$Clicks)
ci_n_clicks
```
```{r}
pass_sanity_check(observed_n_clicks, ci_n_clicks)
```

**Click-through-probability**

```{r}
observed_click_through_probability <- observed_proportion_difference(data_control$Clicks, data_control$Pageviews, data_experiment$Clicks, data_experiment$Pageviews)
observed_click_through_probability
```
```{r}
ci_click_through_probability <- ci_proportion_difference(data_control$Clicks, data_control$Pageviews, data_experiment$Clicks, data_experiment$Pageviews)
ci_click_through_probability
```
```{r}
pass_sanity_check(observed_click_through_probability, ci_click_through_probability)
```

| Invariant Metric          | Lower Bound | Upper Bound | Observed | Passed |
|---------------------------|-------------|-------------|----------|--------|
| Number of cookies         | 0.4988      | 0.5012      | 0.5006   | Yes    |
| Number of clicks          | 0.4959      | 0.5041      | 0.5005   | Yes    |
| Click-through-probability | -0.0013     | 0.0013      | 0.0001   | Yes    |

## Result Analysis

### Effect Size Tests
```{r}
# Inputs:
#   var_control_numerator: A dataframe column containing the control group data numerator
#   var_control_denominator: A dataframe column containing the control group data denominator
#   var_experiment_numerator: A dataframe column containing the experiment group data numerator
#   var_experiment_demoninator: A dataframe column containing the experiment group data denominator
#   alpha: The desired alpha level
# Returns: The 95% confidence interval for the effective size of proportion difference between groups
ci_effective_size <- function(var_control_numerator, var_control_denominator, var_experiment_numerator, var_experiment_demoninator, alpha=0.05) {
  p1 <- sum(var_experiment_numerator) / sum(var_experiment_demoninator)
  p2 <- sum(var_control_numerator) / sum(var_control_denominator)
  d <- p1 - p2
  z_star <- get_z_star(alpha)
  SE <- sqrt(p1*(1-p1)/sum(var_experiment_demoninator) + p2*(1-p2)/sum(var_control_denominator))
  return(c(d - z_star*SE, d + z_star*SE))
}

# Input:
#   ci: The 95% confidence interval of evaluation metric
# Returns: A bool value indicating whether the evaluation metric is statistically significant
check_statistical_significance <- function(ci) {
  return(ci[1]>0 | ci[2]<0)
}

# Input:
#   ci: The 95% confidence interval of evaluation metric
#   dmin: The practical significane level
# Returns: A bool value indicating whether the evaluation metric is statistically significant
check_practical_significance <- function(ci, dmin) {
  return(ci[1]>dmin | ci[2]<(-dmin))
}
  
```

Because both gross conversion and net conversion calculations use columns that contain rows with NA values, I will filter the data to contain only non-NA rows.
```{r}
data_control_filtered <- na.omit(data_control)
data_experiment_filtered <- na.omit(data_experiment)
```

**Gross conversion**

```{r}
ci_gross_conversion <- ci_effective_size(data_control_filtered$Enrollments, data_control_filtered$Clicks, data_experiment_filtered$Enrollments, data_experiment_filtered$Clicks)
ci_gross_conversion
```
```{r}
check_statistical_significance(ci_gross_conversion)
```
```{r}
check_practical_significance(ci_gross_conversion, 0.01)
```

**Net conversion**

```{r}
ci_net_conversion <- ci_effective_size(data_control_filtered$Payments, data_control_filtered$Clicks, data_experiment_filtered$Payments, data_experiment_filtered$Clicks)
ci_net_conversion
```
```{r}
check_statistical_significance(ci_net_conversion)
```
```{r}
check_practical_significance(ci_net_conversion, 0.0075)
```

| Evaluation Metric | Lower Bound | Upper Bound | Statistical Significance | Practical Significance |
|-------------------|-------------|-------------|--------------------------|------------------------|
| Gross conversion  | -0.0291     | -0.0120     | Yes                      | Yes                    |
| Net conversion    | -0.0116     | 0.0019      | No                       | No                     |

### Sign Tests

```{r}
# Inputs:
#   var_control_numerator: A dataframe column containing the control group data numerator
#   var_control_denominator: A dataframe column containing the control group data denominator
#   var_experiment_numerator: A dataframe column containing the experiment group data numerator
#   var_experiment_demoninator: A dataframe column containing the experiment group data denominator
# Returns: Sign test result
sign.test <- function(var_control_numerator, var_control_denominator, var_experiment_numerator, var_experiment_demoninator) {
  n_total <- length(var_control_numerator)
  n_success <- sum(var_experiment_numerator/var_experiment_demoninator > var_control_numerator/var_control_denominator)
  return(binom.test(n_success, n_total)$p.value)
}

# Inputs:
#   p_value: The p value of a given hypothesis test
#   n_tails: 1: one-tailed; 2: two-tailed
#   alpha: The desired alpha level of the test
check_statistical_significance_wtih_p_value <- function(p_value, n_tails=2, alpha=0.05) {
  return(p_value<alpha/n_tails)
}
```

**Gross conversion**
```{r}
p_value_gross_conversion <- sign.test(data_control_filtered$Enrollments, data_control_filtered$Clicks, data_experiment_filtered$Enrollments, data_experiment_filtered$Clicks)
p_value_gross_conversion
```
```{r}
check_statistical_significance_wtih_p_value(p_value_gross_conversion)
```


**Net conversion**
```{r}
p_value_net_conversion <- sign.test(data_control_filtered$Payments, data_control_filtered$Clicks, data_experiment_filtered$Payments, data_experiment_filtered$Clicks)
p_value_net_conversion
```
```{r}
check_statistical_significance_wtih_p_value(p_value_net_conversion)
```

| Evaluation Metric | p-value | Statistical Significance |
|-------------------|---------|--------------------------|
| Gross conversion  | 0.0026  | Yes                      |
| Net conversion    | 0.6776  | No                       |

### Summary

I did not use the Bonferroni correction. The Bonferroni correction is useful to prevent false positives when there are many evaluation metrics. In this experiment, in order to be comfortable to launch the testing feature, we need to meet BOTH of the criteria:

1. There is practically significant decrease of gross conversion in the experiment group.
2. There is NO practically significant decrease of net conversion in the experiment group.

Bonferroni correction is useful if I only require to meet ANY criteria in this experiment to launch the feature. Because I will launch the feature only when BOTH of the criteria are met, I do not need to use the Bonferroni correction.

The effective size tests results agree with the sign tests results.

## Recommendation

I recommend not to launch the experiment.

Our first criterion is met as our data shows that gross conversion of the experiment group is practically significantly lower than that of the control group.

However, regarding our second criterion, the confidence interval of net conversion includes the negative practical significance boundary. This means it is possible that the net conversion might actually have decreased, and the second criterion is not met.

I suggest dig deeper into the data, maybe by looking at some cohorts of each group, or figure out if there are other factors that might have impacted the experiment.


